{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-10T09:38:56.463Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected\n",
      "Removed and ready to reload\n",
      "Connecting to db-file\n",
      "Loading Muons\n",
      "Reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RobustScaler from version 0.19.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data to events\n",
      "   energy_log10  position_x  position_y  position_z  direction_x  direction_y  \\\n",
      "0      2.422178   17.395869  799.810842 -190.892722    -0.449283    -0.652320   \n",
      "1      2.311812  753.894477 -267.662321 -255.052898    -0.865731     0.296605   \n",
      "2      3.433424 -778.158990  185.657174 -176.742064     0.880773    -0.310683   \n",
      "3      2.582473  174.832033  -92.164410  800.000000    -0.100173     0.485243   \n",
      "4      2.783172  -60.428231 -797.714503  388.631793    -0.297639     0.579176   \n",
      "\n",
      "   direction_z  \n",
      "0    -0.610429  \n",
      "1    -0.403157  \n",
      "2    -0.357373  \n",
      "3    -0.868622  \n",
      "4    -0.758924  \n",
      "Generating adjacency matrices\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30040c87a8824649a43adb0836c28fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving dataset\n",
      "Loading data to memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100; Avg_loss: 2.600633: 100%|█████████████████████████████████████████████| 157/157 [03:41<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 done in 221.44 seconds using learning rate: 5.00E-04\n",
      "Avg loss of train: 2.600633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 / 100; Avg_loss: 2.600633:   0%|                                               | 0/157 [03:53<02:51,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss of validation: 1.145767\n",
      "Loss from:  Energy: 0.373114 \t Position: 2.625338 \t Angle: 0.772653 \n",
      "Energy: w = 0.357608 \t Position: u = 2.525871 \t Angle: u = 48.685837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 100; Avg_loss: 0.934586:  36%|████████████████▋                             | 57/157 [05:16<02:13,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "import os, sys, argparse, importlib, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "from tensorflow.python.ops.math_ops import reduce_euclidean_norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb \n",
    "\n",
    "\n",
    "file_path=os.getcwd()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(gpu_devices) > 0:\n",
    "    print(\"GPU detected\")\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from spektral.data import DisjointLoader\n",
    "\n",
    "# file_path = osp.dirname(osp.realpath(__file__))\n",
    "\n",
    "################################################\n",
    "# Setup Deafult Variabls                       # \n",
    "################################################\n",
    "learning_rate = 5e-4\n",
    "batch_size    = 512\n",
    "epochs        = 100\n",
    "early_stop    = True\n",
    "patience      = 20\n",
    "log_wandb     = False\n",
    "model_name    = \"GCN_Schauser_doptimized\"\n",
    "\n",
    "\n",
    "################################################\n",
    "# Setup Hyperparameters                        # \n",
    "################################################\n",
    "hidden_states = 128\n",
    "forward       = False\n",
    "dropout       = 0.5\n",
    "loss_method   = \"loss_func_linear_angle\"\n",
    "n_neighbors   = 6 # SKRIV SELV IND\n",
    "\n",
    "\n",
    "\n",
    "if log_wandb:\n",
    "    wandb.init(project=\"icecube\", entity=\"johannbs\")\n",
    "    # Declare for log\n",
    "    wandb.config.hidden_states = hidden_states\n",
    "    wandb.config.forward = forward\n",
    "    wandb.config.dropout = dropout\n",
    "    wandb.config.learning_rate = learning_rate\n",
    "    wandb.config.batch_size = batch_size\n",
    "    wandb.config.loss_func = loss_method\n",
    "    wandb.config.n_neighbors = n_neighbors\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "# Get model and data                           # \n",
    "################################################\n",
    "from models.GCN_johann import model\n",
    "model = model(n_out=7)\n",
    "# model = load_model(osp.join(file_path, \"models\", \"saved_models\", \"MessPass1\"))\n",
    "\n",
    "\n",
    "\n",
    "import data_load as dl\n",
    "reload(dl)\n",
    "graph_data=dl.graph_data\n",
    "dataset=graph_data(n_data=1e5, restart=1, transform=True, skip=0)\n",
    "idx_lists = dataset.index_lists\n",
    "# Split data\n",
    "dataset_train = dataset[idx_lists[0]]\n",
    "dataset_val   = dataset[idx_lists[1]]\n",
    "dataset_test  = dataset[idx_lists[2]]\n",
    "\n",
    "loader_train = DisjointLoader(dataset_train, epochs=epochs, batch_size=batch_size) # the different loaders work very very differently, beware\n",
    "loader_test = DisjointLoader(dataset_test, batch_size=batch_size, epochs=1)\n",
    "\n",
    "# save_path     = osp.join(file_path, \"models\", \"saved_models\", model_name)\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.mkdir(save_path)\n",
    "\n",
    "################################################\n",
    "# Loss and Optimation                          # \n",
    "################################################\n",
    "# import loss_functions\n",
    "\n",
    "def loss_func_linear_angle(y_reco, y_true, return_from = False):\n",
    "    # Energy loss\n",
    "    loss_energy    = tf.reduce_mean(\n",
    "        tf.abs(\n",
    "            tf.subtract(\n",
    "                y_reco[:,0], y_true[:,0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    loss           = loss_energy\n",
    "\n",
    "    # Position loss\n",
    "    loss_dist  = tf.reduce_mean(\n",
    "        tf.sqrt(\n",
    "            tf.reduce_sum(\n",
    "                tf.square(\n",
    "                    tf.subtract(\n",
    "                        y_reco[:, 1:4], y_true[:, 1:4]\n",
    "                    )\n",
    "                ), axis = 1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # loss       += loss_dist\n",
    "\n",
    "    cos_angle = tf.math.divide_no_nan(tf.reduce_sum(y_reco[:, 4:] * y_true[:, 4:], axis = 1),\n",
    "            tf.math.reduce_euclidean_norm(y_reco[:, 4:], axis = 1) * tf.math.reduce_euclidean_norm(y_true[:, 4:],  axis = 1))\n",
    "\n",
    "    cos_angle -= tf.math.sign(cos_angle) * 1e-6\n",
    "    loss_angle = tf.reduce_mean(tf.math.acos(cos_angle))\n",
    "\n",
    "    loss       += loss_angle\n",
    "    \n",
    "    \n",
    "    if return_from:\n",
    "        return float(loss_energy), float(loss_dist), float(loss_angle)\n",
    "    else:\n",
    "        return loss\n",
    "    \n",
    "loss_func=loss_func_linear_angle\n",
    "\n",
    "opt           = Adam(learning_rate = learning_rate)\n",
    "mse           = MeanSquaredError()\n",
    "\n",
    "def angle(pred, true):\n",
    "    return tf.math.acos(\n",
    "        tf.clip_by_value(\n",
    "            tf.math.divide_no_nan(tf.reduce_sum(pred * true, axis = 1),\n",
    "            tf.math.reduce_euclidean_norm(pred, axis = 1) * tf.math.reduce_euclidean_norm(true,  axis = 1)),\n",
    "            -1., 1.)\n",
    "        )\n",
    "\n",
    "def negative_cos(pred, true):\n",
    "    return 1 - tf.math.divide_no_nan(tf.reduce_sum(pred * true, axis = 1),\n",
    "            tf.math.reduce_euclidean_norm(pred, axis = 1) * tf.math.reduce_euclidean_norm(true,  axis = 1))\n",
    "\n",
    "def metrics(y_reco, y_true):\n",
    "    # Energy metric\n",
    "    energy_residuals = y_true[:, 0] - y_reco[:, 0]\n",
    "    energy_quantiles = tfp.stats.percentile(energy_residuals, [25, 75])\n",
    "    w_energy         = (energy_quantiles[1] - energy_quantiles[0]) / 1.349\n",
    "\n",
    "\n",
    "    # Distanc metric\n",
    "    dist_resi  = tf.sqrt(\n",
    "            tf.reduce_sum(\n",
    "                tf.square(\n",
    "                    tf.subtract(\n",
    "                        y_reco[:, 1:4], y_true[:, 1:4]\n",
    "                    )\n",
    "                ), axis = 1\n",
    "            )\n",
    "        )\n",
    "    u_pos           = tfp.stats.percentile(dist_resi, [68])\n",
    "\n",
    "\n",
    "    # Angle metric\n",
    "    angle_resi = 180 / np.pi * angle(y_reco[:, 4:], y_true[:, 4:])\n",
    "\n",
    "    u_angle         = tfp.stats.percentile(angle_resi, [68])\n",
    "\n",
    "    return float(w_energy.numpy()), float(u_pos.numpy()), float(u_angle.numpy())\n",
    "\n",
    "def lr_schedule(epochs = epochs, initial = learning_rate, decay = 0.9):\n",
    "    n = 1\n",
    "    lr = initial\n",
    "    yield lr\n",
    "    while n < 3:\n",
    "        lr *= 2\n",
    "        n  += 1\n",
    "        yield lr\n",
    "    while True:\n",
    "        lr *= decay\n",
    "        n  += 1 \n",
    "        yield lr\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "################################################\n",
    "# TF - functions                               # \n",
    "################################################\n",
    "\n",
    "@tf.function(input_signature = loader_train.tf_signature(), experimental_relax_shapes = True)\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training = True)\n",
    "        targets     = tf.cast(targets, tf.float32)\n",
    "        loss        = loss_func(predictions, targets)\n",
    "        loss       += sum(model.losses)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function(input_signature = loader_test.tf_signature(), experimental_relax_shapes = True)\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training = False)\n",
    "    targets     = tf.cast(targets, tf.float32) \n",
    "    out         = loss_func(predictions, targets)\n",
    "\n",
    "    return predictions, targets, out\n",
    "\n",
    "\n",
    "def validation(loader):\n",
    "    loss = 0\n",
    "    prediction_list, target_list = [], []\n",
    "    for batch in loader:\n",
    "        inputs, targets = batch\n",
    "        inputs[0][:, :3] = inputs[0][:, :3] / 1000\n",
    "        targets[:, 1:4] = targets[:, 1:4] / 1000\n",
    "        predictions, targets, out = test_step(inputs, targets)\n",
    "        loss           += out\n",
    "        \n",
    "        prediction_list.append(predictions)\n",
    "        target_list.append(targets)\n",
    "    \n",
    "    y_reco  = tf.concat(prediction_list, axis = 0)\n",
    "    y_true  = tf.concat(target_list, axis = 0)\n",
    "    y_true  = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    w_energy, u_pos, u_angle = metrics(y_reco, y_true)\n",
    "    l_energy, l_pos, l_angle = loss_func(y_reco, y_true, return_from = True)\n",
    "    loss                     = loss_func(y_reco, y_true)\n",
    "\n",
    "    return loss, [l_energy, l_pos, l_angle], [w_energy, u_pos, u_angle]\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    loss = 0\n",
    "    prediction_list, target_list = [], []\n",
    "    for batch in loader:\n",
    "        inputs, targets = batch\n",
    "        inputs[0][:, :3] = inputs[0][:, :3] / 1000\n",
    "        targets[:, 1:4] = targets[:, 1:4] / 1000\n",
    "        predictions, targets, out = test_step(inputs, targets)\n",
    "        loss           += out\n",
    "        \n",
    "        prediction_list.append(predictions)\n",
    "        target_list.append(targets)\n",
    "\n",
    "    y_reco  = tf.concat(prediction_list, axis = 0).numpy()\n",
    "    y_true  = tf.concat(target_list, axis = 0)\n",
    "    y_true  = tf.cast(y_true, tf.float32).numpy()\n",
    "\n",
    "\n",
    "    # Unit vects to angles\n",
    "    reco_vects = normalize(y_reco[:, 4:])    \n",
    "    true_vects = normalize(y_true[:, 4:])\n",
    "\n",
    "\n",
    "    reco_azi   = np.arctan2(reco_vects[:, 1], reco_vects[:, 0])\n",
    "    reco_zen   = np.arctan2(reco_vects[:, 2], np.sqrt((reco_vects[:, :2] ** 2).sum(1)))\n",
    "\n",
    "    true_azi   = np.arctan2(true_vects[:, 1], true_vects[:, 0])\n",
    "    true_zen   = np.arctan2(true_vects[:, 2], np.sqrt((true_vects[:, :2] ** 2).sum(1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    energy = y_true[:, 0]\n",
    "    counts, bins = np.histogram(energy, bins = 10)\n",
    "\n",
    "    xs = (bins[1:] + bins[: -1]) / 2\n",
    "\n",
    "    w_energies, u_distances, u_angles = [], [], []\n",
    "\n",
    "    for i in range(len(bins)-1):\n",
    "        idx = np.logical_and(energy > bins[i], energy < bins[i + 1])\n",
    "\n",
    "        w, u_dist, u_angle = metrics(y_true[idx, :], y_reco[idx, :])\n",
    "\n",
    "        w_energies.append(w)\n",
    "        u_distances.append(u_dist)\n",
    "        u_angles.append(u_angle)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols = 3, nrows = 3, figsize = (12, 12))\n",
    "\n",
    "    for a in ax[0]:\n",
    "        a_ = a.twinx()\n",
    "        a_.step(xs, counts, color = \"gray\", zorder = 10, alpha = 0.7, where = \"mid\")\n",
    "        a_.set_yscale(\"log\")\n",
    "        a.set_xlabel(\"Log Energy\")\n",
    "    \n",
    "    ax_top = ax[0]\n",
    "\n",
    "    # Energy reconstruction\n",
    "    ax_top[0].scatter(xs, w_energies)\n",
    "    ax_top[0].set_title(\"Energy Performance\")\n",
    "    ax_top[0].set_ylabel(r\"$w(\\Delta log(E)$\")\n",
    "\n",
    "\n",
    "    # Angle reconstruction\n",
    "    ax_top[1].scatter(xs, u_angles)\n",
    "    ax_top[1].set_title(\"Angle Performance\")\n",
    "    ax_top[1].set_ylabel(r\"$u(\\Delta \\Omega)$\")\n",
    "\n",
    "    # Distance reconstruction\n",
    "    ax_top[2].scatter(xs, u_distances)\n",
    "    ax_top[2].set_title(\"Distance Performance\")\n",
    "    ax_top[2].set_ylabel(r\"$u(||y_{reco} - y_{true}||)$\")\n",
    "\n",
    "\n",
    "    # truth - pred plots\n",
    "    ax_mid = ax[1]\n",
    "\n",
    "    # Energy\n",
    "    ax_mid[0].set_title(\"Energy\")\n",
    "    ax_mid[0].plot(y_true[:, 0], y_reco[:, 0], 'b.', alpha = 0.25)\n",
    "\n",
    "\n",
    "    # Zenith\n",
    "    ax_mid[1].set_title(\"Zenith angle\")\n",
    "    ax_mid[1].plot(true_zen, reco_zen, 'b.', alpha = 0.25)\n",
    "    \n",
    "\n",
    "    # Azimuthal\n",
    "    ax_mid[2].set_title(\"Azimuthal angle\")\n",
    "    ax_mid[2].plot(true_azi, reco_azi, 'b.', alpha = 0.25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Histogram of guesses\n",
    "    ax_bot = ax[2]\n",
    "\n",
    "    # Energy\n",
    "    ax_bot[0].set_title(\"Energy\")\n",
    "    ax_bot[0].hist(y_reco[:, 0] - y_true[:, 0], label = \"reco - true\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[0].hist(y_reco[:, 0], label = \"reco\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[0].hist(y_true[:, 0], label = \"true\", histtype = \"step\", bins = 50)\n",
    "\n",
    "    # Zenith\n",
    "    ax_bot[1].set_title(\"Zenith angle\")\n",
    "    ax_bot[1].hist(reco_zen - true_zen, label = \"reco - true\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[1].hist(reco_zen, label = \"reco\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[1].hist(true_zen, label = \"true\", histtype = \"step\", bins = 50)\n",
    "\n",
    "    # Azimuthal\n",
    "    ax_bot[2].set_title(\"Azimuthal angle\")\n",
    "    ax_bot[2].hist(reco_azi - true_azi, label = \"reco - true\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[2].hist(reco_azi, label = \"reco\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[2].hist(true_azi, label = \"true\", histtype = \"step\", bins = 50)\n",
    "    ax_bot[2].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "################################################\n",
    "# Training                                     # \n",
    "################################################\n",
    "\n",
    "current_batch = 0\n",
    "current_epoch = 1\n",
    "loss          = 0\n",
    "lowest_loss   = 9999999\n",
    "early_stop_counter    = 0\n",
    "\n",
    "pbar          = tqdm(total = loader_train.steps_per_epoch, position = 0, leave = True)\n",
    "start_time    = time.time()\n",
    "lr_gen        = lr_schedule()\n",
    "learning_rate = next(lr_gen)\n",
    "\n",
    "\n",
    "for batch in loader_train:\n",
    "    inputs, targets = batch\n",
    "    inputs[0][:, :3] = inputs[0][:, :3] / 1000\n",
    "    targets[:, 1:4] = targets[:, 1:4] / 1000\n",
    "    out             = train_step(inputs, targets)\n",
    "    loss           += out\n",
    "\n",
    "    current_batch  += 1\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Epoch {current_epoch} / {epochs}; Avg_loss: {loss / current_batch:.6f}\")\n",
    "\n",
    "\n",
    "    if current_batch == loader_train.steps_per_epoch:\n",
    "        \n",
    "        print(f\"Epoch {current_epoch} of {epochs} done in {time.time() - start_time:.2f} seconds using learning rate: {learning_rate:.2E}\")\n",
    "        print(f\"Avg loss of train: {loss / loader_train.steps_per_epoch:.6f}\")\n",
    "\n",
    "        loader_val    = DisjointLoader(dataset_val, epochs = 1,      batch_size = batch_size)\n",
    "        val_loss, val_loss_from, val_metric = validation(loader_val)\n",
    "\n",
    "        print(f\"Avg loss of validation: {val_loss:.6f}\")\n",
    "        print(f\"Loss from:  Energy: {val_loss_from[0]:.6f} \\t Position: {val_loss_from[1]:.6f} \\t Angle: {val_loss_from[2]:.6f} \")\n",
    "        print(f\"Energy: w = {val_metric[0]:.6f} \\t Position: u = {val_metric[1]:.6f} \\t Angle: u = {val_metric[2]:.6f}\")\n",
    "\n",
    "        if log_wandb:\n",
    "            wandb.log({\"Train Loss\":      loss / loader_train.steps_per_epoch,\n",
    "                    \"Validation Loss\": val_loss, \n",
    "                    \"Energy metric\":   val_metric[0],\n",
    "                    \"Position metric\": val_metric[1],\n",
    "                    \"Angle metric\":    val_metric[2],\n",
    "                    \"Learning rate\":   learning_rate})\n",
    "\n",
    "\n",
    "        if val_loss < lowest_loss:\n",
    "            early_stop_counter = 0\n",
    "            lowest_loss        = val_loss\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop and (early_stop_counter >= patience):\n",
    "#             model.save(save_path)\n",
    "            print(f\"Stopped training. No improvement was seen in {patience} epochs\")\n",
    "            break\n",
    "\n",
    "        if current_epoch != epochs:\n",
    "            pbar.n            = 0\n",
    "            pbar.last_print_n = 0\n",
    "            pbar.refresh()\n",
    "\n",
    "        learning_rate = next(lr_gen)\n",
    "        opt.learning_rate.assign(learning_rate)\n",
    "\n",
    "        if current_epoch % 10 == 0:\n",
    "#             model.save(save_path)\n",
    "            print(\"Model saved\")\n",
    "\n",
    "        loss            = 0\n",
    "        start_time      = time.time()\n",
    "        current_epoch  += 1\n",
    "        current_batch   = 0\n",
    "\n",
    "        \n",
    "fig, ax = test(loader_test)\n",
    "fig.savefig(f\"model_tests/{model_name}_test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
