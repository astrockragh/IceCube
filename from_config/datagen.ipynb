{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:15:08.382469Z",
     "start_time": "2021-05-10T23:15:08.357537Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Notebook\n",
      "No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import os, sys, argparse, importlib, time, inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    print('Notebook')\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    print('Not notebook')\n",
    "    from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(gpu_devices) > 0:\n",
    "    print(\"GPU detected\")\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('No GPU detected')\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import spektral\n",
    "from sklearn.preprocessing import normalize\n",
    "from spektral.data import DisjointLoader, BatchLoader, SingleLoader\n",
    "from importlib import reload\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_path='../db_files/dev_lvl7/transformers.pkl'\n",
    "db_path= '../db_files/dev_lvl7/dev_lvl7_mu_nu_e_classification_v003.db'\n",
    "set_path='../db_files/dev_lvl7/sets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_sql, read_pickle, concat, read_csv, DataFrame\n",
    "from sklearn.preprocessing import normalize, RobustScaler\n",
    "from sklearn.neighbors import kneighbors_graph as knn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spektral.data import Dataset, Graph\n",
    "from scipy.sparse import csr_matrix\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "def get_event_no():\n",
    "    print('Reading sets')\n",
    "    sets = read_pickle(set_path)\n",
    "    train_events = sets['train']\n",
    "    test_events = sets['test']\n",
    "    return train_events['event_no'].to_numpy(), test_events['event_no'].to_numpy()\n",
    "features=[\"dom_x\", \"dom_y\", \"dom_z\",  \"dom_time\", \"charge_log10\", \"width\", \"rqe\"]\n",
    "targets= [\"energy_log10\", \"zenith\",\"azimuth\"]\n",
    "\n",
    "n_steps=10\n",
    "n_neighbors=30\n",
    "db_file   = db_path\n",
    "path='processed/where_classic_{n_neighbors}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connecting to db-file\n",
      "Reading files\n",
      "Reading sets\n",
      "Features read\n",
      "All read in 0.81 s, transforming\n",
      "Splitting data to events\n",
      "    event_no    dom_x    dom_y    dom_z  dom_time  charge_log10  width   rqe\n",
      "0   13188817  0.00171 -0.15063 -0.28570 -0.659176     -0.666667    1.0 -0.35\n",
      "14  13188817 -0.01097  0.00672 -0.28315  0.415730      0.000000    1.0  0.00\n",
      "13  13188817 -0.00968 -0.07950 -0.42274 -1.026217      0.083333    1.0  0.00\n",
      "12  13188817 -0.00968 -0.07950 -0.38069  1.007491      0.166667    1.0  0.00\n",
      "10  13188817 -0.00968 -0.07950 -0.28957 -0.350187      1.166667    0.0  0.00\n",
      "   energy_log10    zenith   azimuth  event_no\n",
      "0      2.340637  1.027825  4.888147  13188817\n",
      "1      2.416496  1.107861  4.361933  13188820\n",
      "2      2.409569  1.011339  4.044844  13188832\n",
      "3      2.405540  0.917276  5.294104  13188856\n",
      "4      2.333035  1.197129  1.402828  13188860\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Transform train 0'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84887320c3bf443da4f05daad1eba53f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "List->array\n",
      "Saving dataset train 0: 10000 train\n",
      "Process train 0 took 6.91 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "print(\"Connecting to db-file\")\n",
    "with sqlite3.connect(db_file) as conn:\n",
    "    # Find indices to cut after\n",
    "\n",
    "    # SQL queries format\n",
    "    feature_call = \", \".join(features)\n",
    "    target_call  = \", \".join(targets)\n",
    "\n",
    "    # Load data from db-file\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    train_events1, test_events1=get_event_no()\n",
    "    train_events = np.array_split(train_events1,n_steps)\n",
    "    test_events  = np.array_split(test_events1,n_steps)\n",
    "\n",
    "    for i, (train, test) in enumerate(zip(train_events, test_events)):\n",
    "        if i==0:\n",
    "            for tt, events in zip(['train', 'test'], [train, test]):\n",
    "                if tt=='train':\n",
    "                    events=events[:10000]\n",
    "                    start=time.time()\n",
    "                    df_feat  = read_sql(f\"select event_no, {feature_call} from features where event_no in {tuple(events)}\", conn).sort_values('event_no')\n",
    "                    print('Features read')\n",
    "                    df_targ  = read_sql(f\"select {target_call}, event_no from truth where event_no in {tuple(events)}\", conn).sort_values('event_no')\n",
    "                    stop=time.time()\n",
    "                    print(f'All read in {np.round(stop-start,2)} s, transforming')\n",
    "                    transformers = pickle.load(open(transform_path, 'rb'))\n",
    "                    trans_x      = transformers['features']\n",
    "                    trans_y      = transformers['truth']\n",
    "                    for col in [\"dom_x\", \"dom_y\", \"dom_z\"]:\n",
    "                        df_feat[col] = trans_x[col].inverse_transform(np.array(df_feat[col]).reshape(1, -1)).T/1000\n",
    "\n",
    "                    for col in [\"energy_log10\", \"zenith\",\"azimuth\"]:\n",
    "                        # print(col)\n",
    "                        df_targ[col] = trans_y[col].inverse_transform(np.array(df_targ[col]).reshape(1, -1)).T\n",
    "\n",
    "\n",
    "\n",
    "                    # Cut indices\n",
    "                    print(\"Splitting data to events\")\n",
    "                    idx_list    = np.array(df_feat['event_no'])\n",
    "                    # df_feat.drop('event_no', axis=1, inplace=True)\n",
    "                    x_not_split = np.array(df_feat)\n",
    "\n",
    "                    _, idx = np.unique(idx_list.flatten(), return_index = True) \n",
    "                    xs          = np.split(x_not_split, idx[1:])\n",
    "\n",
    "                    ys          = np.array(df_targ)\n",
    "                    print(df_feat.head())\n",
    "                    print(df_targ.head())\n",
    "\n",
    "                    graph_list=[]\n",
    "                    # Generate adjacency matrices\n",
    "                    for x, y in tqdm(zip(xs, ys), total = len(xs), position=1, desc=f'Transform {tt} {i}'):\n",
    "                        try:\n",
    "                            a = knn(x[:, :3], n_neighbors)\n",
    "                        except:\n",
    "                            a = csr_matrix(np.ones(shape = (x.shape[0], x.shape[0])) - np.eye(x.shape[0]))\n",
    "                        graph_list.append(Graph(x = x, a = a, y = y))\n",
    "                    print('List->array')\n",
    "                    graph_list = np.array(graph_list, dtype = object)\n",
    "                    print(f\"Saving dataset {tt} {i}: {len(graph_list)} {tt}\")\n",
    "                    pickle.dump(graph_list, open(osp.join(path, f\"{tt}_{i}.dat\"), 'wb'))\n",
    "                    # pickle.dump(graph_list, open(osp.join('processed/debug', f\"{tt}_{i}.dat\"), 'wb'))\n",
    "                    stop=time.time()\n",
    "                    print(f\"Process {tt} {i} took {np.round(stop-start, 2)} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns=[]\n",
    "for x in xs:\n",
    "    lns.append(len(np.unique(x[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(lns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(np.unique(ys[:,3])==np.unique(x_not_split[:,7]))\n",
    "# np.sum(np.in1d(np.unique(x_not_split[:,7]), test_events1))\n",
    "data  = pickle.load(open(osp.join(path, f\"train_{0}.dat\"), 'rb'))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2f11cda273c4ad596e0b37639a97a20"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enoy=[]\n",
    "enox=[]\n",
    "lenox=[]\n",
    "doms=[]\n",
    "for i in tqdm(range(len(data))):\n",
    "    enoy.append(data[i].y[0])\n",
    "    lenox.append(len(np.unique(data[i].x[:,0])))\n",
    "    enox.append(np.unique(data[i].x[:,0]))\n",
    "    doms.append(len(data[i].x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:16:11.739872Z",
     "start_time": "2021-05-10T23:15:08.666416Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed and ready to reload\n",
      "Connecting to db-file\n",
      "Reading files\n",
      "Reading sets\n",
      "Features read\n",
      "Targets read, transforming\n",
      "Reading train 0 took 349.35 s\n",
      "Splitting data to events\n",
      "           dom_x    dom_y    dom_z  dom_time  charge_log10  width   rqe\n",
      "7989078  0.11319 -0.06047 -0.25221  1.812734     -0.750000    1.0  0.00\n",
      "7989069  0.12497 -0.13125 -0.40845  1.058052      1.333333    1.0 -0.35\n",
      "7989070  0.07237 -0.06660 -0.30000  1.528090      0.166667    1.0  0.00\n",
      "7989071  0.07237 -0.06660 -0.31402 -0.546816      0.333333    0.0  0.00\n",
      "7989072  0.07237 -0.06660 -0.32103  0.837079      0.750000    0.0  0.00\n",
      "        energy_log10    zenith   azimuth  event_no\n",
      "303764      0.421948  2.676098  1.044445         1\n",
      "303765      0.463034  0.693687  6.133094         6\n",
      "303766      0.671135  2.486230  5.891520        37\n",
      "303767      0.566209  1.957056  0.632692        39\n",
      "303768      0.568009  1.858723  2.070458        40\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Transform train 0'), FloatProgress(value=0.0, max=663345.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58dcca5e318049ed94a8d73b642c5229"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "List->array\n",
      "Saving dataset train 0: 663345 train\n",
      "Process train 0 took 783.82 s\n",
      "Features read\n"
     ]
    }
   ],
   "source": [
    "import dev.datawhere as dl\n",
    "reload(dl)\n",
    "graph_data=dl.graph_data\n",
    "dataset=graph_data(n_steps=10, n_neighbors=30,\n",
    "        transform_path='../db_files/dev_lvl7/transformers.pkl',\\\n",
    "             db_path= '../db_files/dev_lvl7/dev_lvl7_mu_nu_e_classification_v003.db', restart=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:36:57.156381Z",
     "start_time": "2021-05-10T23:36:57.141420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:06:44.295959Z",
     "start_time": "2021-05-10T23:06:44.280998Z"
    }
   },
   "outputs": [],
   "source": [
    "df=dataset.df_event\n",
    "test=np.arange(0,10000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:06:49.504779Z",
     "start_time": "2021-05-10T23:06:49.487824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=9, n_node_features=7, n_edge_features=None, n_labels=3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:07:36.739399Z",
     "start_time": "2021-05-10T23:07:36.724440Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_lists = [np.array(df[df['event_no'].isin(test)].index)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:07:37.150449Z",
     "start_time": "2021-05-10T23:07:37.134492Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test=dataset[idx_lists[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:06:53.628099Z",
     "start_time": "2021-05-10T23:06:53.610147Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = DisjointLoader(dataset, epochs=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:48:41.044638Z",
     "start_time": "2021-05-10T21:48:40.656410Z"
    }
   },
   "outputs": [],
   "source": [
    "path='processed/submit_muon_0_n_data_10000_type_classic_nn_30/data.npy'\n",
    "data=np.load(path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:07:39.768433Z",
     "start_time": "2021-05-10T23:07:39.764443Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = DisjointLoader(dataset_test, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:07:41.060112Z",
     "start_time": "2021-05-10T23:07:41.037174Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature = loader.tf_signature(), experimental_relax_shapes = True)\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training = True)\n",
    "        targets     = tf.cast(targets, tf.float32)\n",
    "        loss        = loss_func(predictions, targets)\n",
    "        loss       += sum(model.losses)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:07:44.105462Z",
     "start_time": "2021-05-10T23:07:44.081526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'spec': tensorflow.python.framework.tensor_spec.TensorSpec,\n",
       "  'shape': (None, 7),\n",
       "  'dtype': tf.float64},\n",
       " 'a': {'spec': tensorflow.python.framework.sparse_tensor.SparseTensorSpec,\n",
       "  'shape': (None, None),\n",
       "  'dtype': tf.float64},\n",
       " 'y': {'spec': tensorflow.python.framework.tensor_spec.TensorSpec,\n",
       "  'shape': (3,),\n",
       "  'dtype': tf.float64}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.dataset.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T23:05:04.127394Z",
     "start_time": "2021-05-10T23:05:04.118418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Graph(n_nodes=20, n_node_features=7, n_edge_features=None, n_labels=3)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ee7ae0cce42568ffbe792829a147e8b26e32fc5929320da4cac6b0f8a68675f6",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ee7ae0cce42568ffbe792829a147e8b26e32fc5929320da4cac6b0f8a68675f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}